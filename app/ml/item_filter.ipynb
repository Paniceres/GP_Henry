{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bruno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando nltk para procesar las categorias, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Funcion que realiza tokeinizacion en base a un texto.\n",
    "\n",
    "    Args:\n",
    "        text (string): Palabra u oraci√≥n para aplicar la tokeinizacin.\n",
    "\n",
    "    Returns:\n",
    "        str: Serie de strings.\n",
    "    \"\"\"\n",
    "    # Aplico la teokeinizacion\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    words = [ps.stem(word.lower()) for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "def obtener_palabras_similares(palabra, modelo, topn=3):\n",
    "    try:\n",
    "        similares = modelo.similar_by_word(palabra, topn=topn)\n",
    "        return [palabra for palabra, _ in similares]\n",
    "    except KeyError:\n",
    "        return []\n",
    "\n",
    "def categories_nlp():  \n",
    "    \"\"\"\n",
    "    Funcion que a partir de un dataframe con categorias de columna \"name\" aplica la funcion *process_text*\n",
    "\n",
    "    Returns:\n",
    "        TfidfVectorizer: Matriz TF-IDF represetando  la frecuencia de terminos ponderada por importancia.(necesaria para el modelo.)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Genero un dataframe que contenga, las categorias y los negocios para yelp y google.\n",
    "\n",
    "    # Cambiar por la lectura a la BD\n",
    "\n",
    "    local_categories_google = pd.read_parquet('../../datasets/processed/bd/7_categories_google.parquet.gz')\n",
    "\n",
    "    # Cambiar por la lectura a la BD\n",
    "    local_categories_yelp = pd.read_parquet('../../datasets/processed/bd/8_categories_yelp.parquet.gz')\n",
    "\n",
    "    #Si se lee de la base de datos business_id ya esta como nombre.\n",
    "    local_categories_google.rename(columns={'gmap_id':'business_id'},inplace=True)\n",
    "    local_categories = pd.concat([local_categories_google,local_categories_yelp])\n",
    "\n",
    "    # Cambiar por la lectura a la BD\n",
    "    categoires = pd.read_parquet('../../datasets/processed/bd/2_categories.parquet.gz')\n",
    "    local_categories = pd.merge(local_categories,categoires,on='categories_id',how='inner')\n",
    "    \n",
    "    #### Se genera el dataframe local_categories.#####\n",
    "    \n",
    "    \n",
    "    \n",
    "    local_categories['procceced'] = local_categories['name'].apply(process_text)\n",
    "\n",
    "    # Si hay mas clase ademas de restaur ej: pizza restaur borra restaur, si no deja igual\n",
    "    local_categories['procceced'] = local_categories['procceced'].apply(lambda x:x.replace('restaur','') if x!= 'restaur' else x)\n",
    "    local_categories['procceced'] = local_categories['procceced'].astype(str)\n",
    "    # Crear una matriz TF-IDF para medir la similitud del contenido\n",
    "    \n",
    "    from gensim.models import KeyedVectors\n",
    "\n",
    "    # Ruta al archivo GoogleNews-vectors-negative300.bin\n",
    "    ruta_modelo = '../../datasets/extras/model/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "    # Cargar el modelo\n",
    "    modelo = KeyedVectors.load_word2vec_format(ruta_modelo, binary=True,limit=500000)\n",
    "    \n",
    "    \n",
    "    local_categories['processed'] = local_categories['procceced'].apply(\n",
    "    lambda text: ' '.join(\n",
    "        [\n",
    "            ' '.join(obtener_palabras_similares(palabra.strip(), modelo)) \n",
    "            if palabra in text \n",
    "            else palabra \n",
    "            for palabra in text.split()\n",
    "        ]\n",
    "    )\n",
    "    )   \n",
    "    \n",
    "    local_categories = local_categories[['business_id','name','processed']]\n",
    "    local_categories.to_parquet('./datasets/locales_categories.parquet') # Guardo el dataset util\n",
    "\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(local_categories['processed'])\n",
    "    return tfidf_matrix\n",
    "\n",
    "    #Proceso para normalizar las categorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gnero las categorias para luego procesarlas y guardarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero un dataframe que contenga, las categorias y los negocios para yelp y google.\n",
    "\n",
    "# Cambiar por la lectura a la BD\n",
    "\n",
    "local_categories_google = pd.read_parquet('../../datasets/processed/bd/7_categories_google.parquet.gz')\n",
    "\n",
    "# Cambiar por la lectura a la BD\n",
    "local_categories_yelp = pd.read_parquet('../../datasets/processed/bd/8_categories_yelp.parquet.gz')\n",
    "\n",
    "#Si se lee de la base de datos business_id ya esta como nombre.\n",
    "local_categories_google.rename(columns={'gmap_id':'business_id'},inplace=True)\n",
    "local_categories = pd.concat([local_categories_google,local_categories_yelp])\n",
    "\n",
    "# Cambiar por la lectura a la BD\n",
    "categoires = pd.read_parquet('../../datasets/processed/bd/2_categories.parquet.gz')\n",
    "local_categories = pd.merge(local_categories,categoires,on='categories_id',how='inner')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Hago el procesamiento de las categorias con NLTK y los exporto en un pkl\n",
    "categories_procceced = categories_nlp() \n",
    "with open('./tfidf_matrix.pkl', 'wb') as file:\n",
    "        pickle.dump(categories_procceced, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUACION DEL MODELO\n",
    "\n",
    "\n",
    "# Cargo la matriz generada del procesamiento\n",
    "with open('./tfidf_matrix.pkl', 'rb') as file:\n",
    "        tfidf_matrix = pickle.load(file)\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(tfidf_matrix, test_size=0.2, random_state=42)\n",
    "\n",
    "knn_model = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=30)\n",
    "\n",
    "knn_model.fit(tfidf_matrix)\n",
    "local_categories = pd.read_parquet('./app/ml/datasets/locales_categories.parquet')\n",
    "idx = local_categories[local_categories['business_id'] == business_id].index[0]\n",
    "\n",
    "_, indices = knn_model.kneighbors(categories_procceced[idx])\n",
    "recommendations = local_categories['business_id'].iloc[indices[0][1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo de recomendacion usando similitudes con vecinos cercanos\n",
    "\n",
    "\n",
    "# Cargo la matriz generada del procesamiento\n",
    "with open('./tfidf_matrix.pkl', 'rb') as file:\n",
    "        tfidf_matrix = pickle.load(file)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Defino y entreno al modelo.\n",
    "knn_model = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=30)\n",
    "knn_model.fit(tfidf_matrix)\n",
    "\n",
    "# Guardo el modelo en un pkl\n",
    "with open('./modelo_knn.pkl', 'wb') as file:\n",
    "    pickle.dump(knn_model, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTRO POR DISTANCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que calcula la distancia entre dos punto en funcion de las coordenadas\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta funcion aplica la distancia hervesine para encontrar la distancia entre dos puntos a partir de sus coordenaadas.\n",
    "    \n",
    "    Args:\n",
    "        lat1 (float): Latitud del primer punto.\n",
    "        lon1 (float): Longitud del primer punto\n",
    "        lat2 (float: Latitud del segundo punto.\n",
    "        lon2 (float): Longitud del segundo punto.\n",
    "        \n",
    "    Returns:\n",
    "        float:Distancia en metros entre dos coordeandas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Radio de la Tierra en kilometros\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convierte las coordenadas de grados a radianes\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Diferencia de latitud y longitud\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # F√≥rmula haversine\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Distancia en metros\n",
    "    distance = R * c\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que a partir de un id de negocio y una lista ids retorna la distancia entre ese negocio y cada uno de los demas\n",
    "def distance(business_id,business_id_list,rang=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta funcion calcula a partir de un negocio y una lista de negocios, la distancia entre los puntos.\n",
    "\n",
    "    Args:\n",
    "        business_id(str): Id del negocio.\n",
    "        business_id_list(list): Lista con id de negocios.\n",
    "        rang(float,optional) : Maxima distancia(kilometros) sobre la cual se quiere devolver los negocios.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame de los negocios tomando en cuenta las distnacias\n",
    "    \"\"\"\n",
    "    \n",
    "    if rang:\n",
    "        filtro_distance = rang\n",
    "    else:   \n",
    "        filtro_distance = 300000000 #FIltro distancia en metros.\n",
    "    #Genero un dataframe con los restaurantes de google y yelp\n",
    "    \n",
    "    # Cambiar por la lectura a la BD, si se lee de ahi business_id ya esta como nombre\n",
    "    business_google=pd.read_parquet('../../datasets/processed/bd/5_business_google.parquet.gz') \n",
    "    business_google.rename(columns={'gmap_id':'business_id'},inplace=True) \n",
    "    \n",
    "    # Cambiar por la lectura a la BD\n",
    "    business_yelp=pd.read_parquet('../../datasets/processed/bd/6_business_yelp.parquet.gz') \n",
    "    \n",
    "    # si se lee de la base de datos cambiar stars de business_yelp por avg_stars\n",
    "    business = pd.concat([business_google[['business_id','name','avg_stars','latitude','longitude','state_id']],business_yelp[['business_id','name','avg_stars','latitude','longitude','state_id']]])\n",
    "    #Genero las coordenadas del local al que le quiero encontrar recomendaciones.\n",
    "    lat_origin,long_origin = business[business['business_id']==business_id]['latitude'].iloc[0],business[business['business_id']==business_id]['longitude'].iloc[0]\n",
    "    #Filtro solo por los restaurantes que pertenecen a las recomendaciones.\n",
    "    business = business[business['business_id'].isin(business_id_list)]\n",
    "    #Calculo la distancia de cada restuarante recomendado al inicial\n",
    "    business['distance'] = business.apply(lambda row: haversine(lat_origin, long_origin, row['latitude'], row['longitude']), axis=1)\n",
    "    #Aplico el filtro de distancia.\n",
    "    business = business[business['distance']<filtro_distance]\n",
    "    return business\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para obtener recomendaciones\n",
    "def get_recommendations(business_id,rang=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Funcion que a partir de un negocio, recomienda otros, en funcion de sus categorias usando el modelo KNN.\n",
    "    \n",
    "    Ags:\n",
    "        business_id(str) : id del negocio al cual se le quieren calcular recomendaciones.\n",
    "        rang(float,optional) :Rango de distancia para obtener las recomendaciones(kilometros).\n",
    "\n",
    "    Returns:\n",
    "        business_cat(pd.DataFrame):Data Frame con las recomendaciones junto no algunas caracteristicas del negocio.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cargo el modelo\n",
    "    with open('./modelo_knn.pkl', 'rb') as file: \n",
    "        knn_model = pickle.load(file)\n",
    "        \n",
    "    with open('./tfidf_matrix.pkl', 'rb') as file:\n",
    "        categories_procceced = pickle.load(file)\n",
    "    \n",
    "    ######### categories_procceced podria ser un df importado  con todos los pasos anteriores.#########\n",
    "    local_categories = pd.read_parquet('./datasets/locales_categories.parquet')\n",
    "\n",
    "    idx = local_categories[local_categories['business_id'] == business_id].index[0]\n",
    "   \n",
    "    \n",
    "    \n",
    "    #Genero las recomendaciones.\n",
    "    _, indices = knn_model.kneighbors(categories_procceced[idx])\n",
    "    recommendations = local_categories['business_id'].iloc[indices[0][0:]]  # Excluye el propio restaurante\n",
    "    \n",
    "    #Calcula las distancias entre las recomendaciones y el local.\n",
    "    if rang:\n",
    "        business = distance(business_id,recommendations,rang)\n",
    "    else:\n",
    "        business = distance(business_id,recommendations)\n",
    "    business = business[business['distance']!=0.0] # Elimino al restaurante mismos.\n",
    "    #Uno las caractereisticas de los locales, con las categorias.\n",
    "    business_cat = pd.merge(local_categories,business,on='business_id')\n",
    "    business_cat = business_cat.groupby('business_id').agg({\n",
    "        'latitude':'first',\n",
    "        'longitude':'first',\n",
    "        'name_x':list,\n",
    "        'name_y':'first',\n",
    "        'distance':'first',\n",
    "        'avg_stars':'mean',\n",
    "        'state_id':'first'\n",
    "        \n",
    "    }).reset_index().rename(columns =({'name_x':'category','name_y':'name'}))\n",
    "        \n",
    "    \n",
    "    return business_cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTRO POR USUARIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que recibe un business id userid o categoria y recomienda locales, tambien puede agregarse el rango en metros de distancia.\n",
    "def recommendation(business_ids=None,user_id=None,category=None,distance=None,state=None):\n",
    "    \"\"\"\n",
    "    Esta funcion a partird e un negocio usuario o categoria recomienda otros negocios, teniendo en cuenta la distancia de ser requerida.\n",
    "    Para esto la funcion toma un negocio, o selecciona una lista de ellos usando user_id, y categorias, y aplica la funcion *get_recommendations*\n",
    "\n",
    "    Args:\n",
    "        business_ids (str, optional): Id de un negocio.\n",
    "        user_id (str, optional): Id de un usuario.\n",
    "        category (str, optional): Categoria (nombre).\n",
    "        distance (float, optional): Distancia en kilometros.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data Frame con √±as recomendaciones y otras caracteristicas(analizar el uso de json)\n",
    "    \"\"\"\n",
    "        \n",
    "    if business_ids:\n",
    "        business_ids = [business_ids]\n",
    "    \n",
    "    if user_id:\n",
    "        \n",
    "        # Cambiar por la lectura a la BD\n",
    "        df_rg = pd.read_parquet('../../datasets/processed/bd/9_reviews_google.parquet.gz',columns=['user_id','gmap_id','sentiment'])\n",
    "        df_ry = pd.read_parquet('../../datasets/processed/bd/10_reviews_yelp.parquet.gz',columns=['user_id','business_id','sentiment'])\n",
    "        df = pd.concat([df_rg,df_ry])\n",
    "        business_ids = df[df['user_id']==user_id].iloc[:10]['business_id'].tolist()\n",
    "        distance = None\n",
    "        if len(business_ids) == 0:\n",
    "            return 'Usuario no encontrado.'\n",
    "        \n",
    "    if category:\n",
    "        df_categories = pd.read_parquet('./datasets/locales_categories.parquet')\n",
    "        business_ids = df_categories[df_categories['name'].str.lower().str.contains(category.lower())].sample(10).iloc[:10]['business_id'].tolist()\n",
    "        distance = None\n",
    "        if len(business_ids) == 0:\n",
    "            return 'Categoria no encontrada.'\n",
    "        \n",
    "        \n",
    "    business_cat = pd.DataFrame()\n",
    "    \n",
    "    for business_id in business_ids:\n",
    "        business_cat = pd.concat([get_recommendations(business_id,rang=distance),business_cat])    \n",
    "        \n",
    "    if business_cat.shape[0] == 0:\n",
    "        return 'Restaurante no encontrado.'\n",
    "    \n",
    "    states = pd.read_parquet('../../datasets/processed/bd/1_states.parquet.gz')\n",
    "    business_cat = pd.merge(business_cat,states,on='state_id',how='inner')\n",
    "    business_cat = business_cat[['business_id','name','category','state','latitude','longitude','avg_stars','distance']]\n",
    "    if state:\n",
    "        business_cat = business_cat[business_cat['state']==state]\n",
    "        \n",
    "    return business_cat.sort_values(by=['distance','avg_stars'],ascending=[True,False]).iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>avg_stars</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [gmap_id, name, latitude, longitude, avg_stars, state_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_google=pd.read_parquet('../../datasets/processed/bd/5_business_google.parquet.gz') \n",
    "\n",
    "business_google[business_google['gmap_id'] == '0x809042411231551b:0x891040c09bbc3f9b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/processed/bd/7_categories_google.parquet.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Henry_PF\\GP_Henry\\app\\ml\\item_filter.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m local_categories_google \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(\u001b[39m'\u001b[39;49m\u001b[39m../datasets/processed/bd/7_categories_google.parquet.gz\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m local_categories_google\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[0;32m    671\u001b[0m     path,\n\u001b[0;32m    672\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m    673\u001b[0m     filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[0;32m    674\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    675\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[0;32m    676\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    677\u001b[0m     filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[0;32m    678\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    679\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    263\u001b[0m     to_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39msplit_blocks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    266\u001b[0m     path,\n\u001b[0;32m    267\u001b[0m     filesystem,\n\u001b[0;32m    268\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    269\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    270\u001b[0m )\n\u001b[0;32m    271\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     pa_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[0;32m    273\u001b[0m         path_or_handle,\n\u001b[0;32m    274\u001b[0m         columns\u001b[39m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    278\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    129\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    131\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[0;32m    132\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m    140\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n\u001b[0;32m    873\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/processed/bd/7_categories_google.parquet.gz'"
     ]
    }
   ],
   "source": [
    "local_categories_google = pd.read_parquet('../datasets/processed/bd/7_categories_google.parquet.gz')\n",
    "local_categories_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Henry_PF\\GP_Henry\\app\\ml\\item_filter.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m recommendation(business_ids\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpizza\u001b[39;49m\u001b[39m'\u001b[39;49m,distance\u001b[39m=\u001b[39;49m\u001b[39m100000.0\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Henry_PF\\GP_Henry\\app\\ml\\item_filter.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m business_cat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m business_id \u001b[39min\u001b[39;00m business_ids:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     business_cat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([get_recommendations(business_id,rang\u001b[39m=\u001b[39;49mdistance),business_cat])    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m business_cat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mRestaurante no encontrado.\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32mc:\\Henry_PF\\GP_Henry\\app\\ml\\item_filter.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m######### categories_procceced podria ser un df importado  con todos los pasos anteriores.#########\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m local_categories \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39m'\u001b[39m\u001b[39m./datasets/locales_categories.parquet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m idx \u001b[39m=\u001b[39m local_categories[local_categories[\u001b[39m'\u001b[39;49m\u001b[39mbusiness_id\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m business_id]\u001b[39m.\u001b[39;49mindex[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#Genero las recomendaciones.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Henry_PF/GP_Henry/app/ml/item_filter.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m _, indices \u001b[39m=\u001b[39m knn_model\u001b[39m.\u001b[39mkneighbors(categories_procceced[idx])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:5365\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5362\u001b[0m \u001b[39mif\u001b[39;00m is_integer(key) \u001b[39mor\u001b[39;00m is_float(key):\n\u001b[0;32m   5363\u001b[0m     \u001b[39m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[0;32m   5364\u001b[0m     key \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mcast_scalar_indexer(key)\n\u001b[1;32m-> 5365\u001b[0m     \u001b[39mreturn\u001b[39;00m getitem(key)\n\u001b[0;32m   5367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[0;32m   5368\u001b[0m     \u001b[39m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[0;32m   5369\u001b[0m     \u001b[39m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[0;32m   5370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "recommendation(business_ids='pizza',distance=100000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHORA INTENTAMOS CON WORDVEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Ruta al archivo GoogleNews-vectors-negative300.bin\n",
    "ruta_modelo = '../../datasets/extras/model/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Cargar el modelo\n",
    "modelo = KeyedVectors.load_word2vec_format(ruta_modelo, binary=True,limit=500000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_palabras_similares(palabra, modelo, topn=3):\n",
    "    try:\n",
    "        similares = modelo.similar_by_word(palabra, topn=topn)\n",
    "        return [palabra for palabra, _ in similares]\n",
    "    except KeyError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obtener_palabras_similares('restaurant',modelo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_categories.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_categories['processed'] = local_categories['procceced'].apply(\n",
    "    lambda text: ' '.join(\n",
    "        [\n",
    "            ' '.join(obtener_palabras_similares(palabra.strip(), modelo)) \n",
    "            if palabra in text \n",
    "            else palabra \n",
    "            for palabra in text.split()\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_categories.loc[local_categories['processed'] == '', 'processed'] = 'restaur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_categories.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(local_categories['processed'])\n",
    "\n",
    "\n",
    "\n",
    "#Defino y entreno al modelo.\n",
    "knn_model = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10)\n",
    "knn_model.fit(tfidf_matrix)\n",
    "\n",
    "# Guardo el modelo en un pkl\n",
    "with open('./model/modelo_knn.pkl', 'wb') as file:\n",
    "    pickle.dump(knn_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero las recomendaciones.\n",
    "_, indices = knn_model.kneighbors(tfidf_matrix[106106])\n",
    "recommendations = local_categories['business_id'].iloc[indices[0][1:]]  # Excluye el propio restaurante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_categories.iloc[106106])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_categories[local_categories['business_id']=='0x88d905f9b0dccd0d:0x189d7a76056de69a']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
