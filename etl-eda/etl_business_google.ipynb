{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.point import Point\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Bussiness Google</center></h1>\n",
    "\n",
    "<h1><center>Lectura de información</center></h1>\n",
    "\n",
    "El primer paso antes de realizar la normalización y limpieza de los datasets correspondientes a los metadatos de google, fue unificar los ya mencionados datasets para luego proceder a realizar la exportación de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este bloque, realize la apertura de todos los .json de meta datos de google, para luego concatenarlos y formar un solo dataframe con toda la informacion\n",
    "'''\n",
    "dicc = {}\n",
    "\n",
    "for x in range(1,12):\n",
    "    dicc[x] = pd.read_json(f'extras\\Google Maps\\metadata-sitios\\{x}.json', lines=True)\n",
    "    \n",
    "# Lista para almacenar los DataFrames\n",
    "lista_dataframes = []\n",
    "\n",
    "# Iterar sobre los valores del diccionario (los Datasets<.json>)\n",
    "for df in dicc.values():\n",
    "    lista_dataframes.append(df)\n",
    "\n",
    "# Concatenar los DataFrames en uno solo\n",
    "resultado_concatenado = pd.concat(lista_dataframes, ignore_index=True)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabe aclarar que este bloque de comandos solo se utiliza en primera instancia, ya que luego se trabajara sobre la exportación unificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resultado_concatenado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de dataset unificado  \n",
    "meta_datos_google = pd.read_json(r'..\\extras\\google Maps\\metadata-sitios\\bussiness_google.json.gz')\n",
    "\n",
    "meta_datos_google.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultado_concatenado.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columnas eliminadas\n",
    "\n",
    "-state (porque no es relevante, dice si esta abierto el local o no)\n",
    "\n",
    "-url (se puede ingresar utilizando la siguiente url mas el gmap-id) [https://www.google.com/maps/place//data=!4m2!3m1!1s {aca va el gmap_id}]\n",
    "\n",
    "-description (esta información podemos obtenerla en caso de ser necesario usando la api, solo tiene 250.000 datos lo que vuelve a la columna irrelevante en comparación con la cantidad total, por eso mismo se recomienda realizar la extracción de dicha columna de ser necesario desde la api)\n",
    "\n",
    "-MISC (esta columna tiene una descripcion de como estan las condiciones del local)\n",
    "\n",
    "-price (es irrelevante el precio)\n",
    "\n",
    "-num_of_reviews (esta columna tendrémos que volver a crearla luego de realizar el ETL de los estados)\n",
    "\n",
    "-address (esta información podemos extraerla del url en el caso de ser necesario)\n",
    "\n",
    "-hours (en caso de ser necesario se extraeran los datos específicos de la url)\n",
    "\n",
    "-relative_result (podemos obtener el mismo resultado y hasta mas eficiente realizando una ponderación de los valores de latitude y longitude)\n",
    "\n",
    "Muestra de acceso a google maps por medio de url mas gmap_id:\n",
    "\n",
    "https://www.google.com/maps/place//data=!4m2!3m1!1s0x88e77e406e2da09d:0x479c876de5fd4ec4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_concatenado.drop(columns=['state','url','description','MISC','price','num_of_reviews','address','hours','relative_results'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_concatenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bussiness_google = pd.read_json(r'..\\datasets\\processed\\google\\bussiness_google.json.gz')\n",
    "bussiness_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illinois = gpd.read_file(r'..\\datasets\\raw\\IL.geo.json')\n",
    "# Crear un GeoDataFrame a partir del DataFrame existente\n",
    "geometry = [Point(xy) for xy in zip(bussiness_google['longitude'], bussiness_google['latitude'])]\n",
    "meta_datos_google = gpd.GeoDataFrame(bussiness_google, geometry=geometry)\n",
    "# Realizar la unión espacial\n",
    "empresas_en_illinois_google = gpd.sjoin(meta_datos_google, illinois, op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = gpd.read_file(r'..\\datasets\\raw\\CA.geo.json')\n",
    "# Crear un GeoDataFrame a partir del DataFrame existente\n",
    "geometry = [Point(xy) for xy in zip(bussiness_google['longitude'], bussiness_google['latitude'])]\n",
    "meta_datos_google = gpd.GeoDataFrame(bussiness_google, geometry=geometry)\n",
    "# Realizar la unión espacial\n",
    "empresas_en_california_google = gpd.sjoin(meta_datos_google, california, op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "florida = gpd.read_file(r'..\\datasets\\raw\\FL.geo.json')\n",
    "# Crear un GeoDataFrame a partir del DataFrame existente\n",
    "geometry = [Point(xy) for xy in zip(bussiness_google['longitude'], bussiness_google['latitude'])]\n",
    "meta_datos_google = gpd.GeoDataFrame(bussiness_google, geometry=geometry)\n",
    "# Realizar la unión espacial\n",
    "empresas_en_florida_google = gpd.sjoin(meta_datos_google, florida, op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nuevajersey = gpd.read_file(r'..\\datasets\\raw\\NJ.geo.json')\n",
    "# Crear un GeoDataFrame a partir del DataFrame existente\n",
    "geometry = [Point(xy) for xy in zip(bussiness_google['longitude'], bussiness_google['latitude'])]\n",
    "meta_datos_google = gpd.GeoDataFrame(bussiness_google, geometry=geometry)\n",
    "# Realizar la unión espacial\n",
    "empresas_en_Nuevajersey_google = gpd.sjoin(meta_datos_google, Nuevajersey, op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(empresas_en_florida_google) + len(empresas_en_california_google) + len(empresas_en_illinois_google) + len(empresas_en_Nuevajersey_google))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de DataFrames a concatenar\n",
    "dataframes = [empresas_en_florida_google, empresas_en_california_google, empresas_en_illinois_google, empresas_en_Nuevajersey_google]\n",
    "\n",
    "# Concatenar los DataFrames verticalmente\n",
    "empresas_concatenadas_google = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de nombres de columnas a eliminar\n",
    "columnas_a_eliminar = ['geometry', 'index_right', 'id', 'fips','Estado_Perteneciente']\n",
    "\n",
    "# Eliminar las columnas de empresas_concatenadas\n",
    "empresas_concatenadas_google = empresas_concatenadas_google.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "empresas_concatenadas_google.rename(columns={'name_right': 'state'}, inplace=True)\n",
    "empresas_concatenadas_google.rename(columns={'name_left': 'name'}, inplace=True)\n",
    "empresas_concatenadas_google.rename(columns={'avg_rating': 'stars'}, inplace=True)\n",
    "\n",
    "empresas_concatenadas_google.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo listas para utilizar en las siguientes iteraciones\n",
    "categorias = []\n",
    "filtro_cat = []\n",
    "restaurant_cat = []\n",
    "\n",
    "# Itero por la columna categories del dataframe, para extraer todos sus valores y convertirlos en listas, los que luego agrego a la lista categorias\n",
    "for x in empresas_concatenadas_google['category']:\n",
    "    alm = x.split(\"'\")\n",
    "    categorias.append(alm)\n",
    "    \n",
    "# Realizo una limpieza de cada lista dentro de la lista categorias\n",
    "for x in categorias:\n",
    "    for i in x:\n",
    "        if len(i) > 3:\n",
    "            filtro_cat.append(i)\n",
    "\n",
    "# Itero sobre cada fila de la columna categories, con la finalidad de filtrar todos los comercios que corresponden a restaurantes\n",
    "for x in filtro_cat:\n",
    "    if 'estau' in x:\n",
    "        restaurant_cat.append(x)\n",
    "\n",
    "# Con esta conversion elimino los valores repetidos de toda mi lista\n",
    "restaurant_cat = set(restaurant_cat)\n",
    "\n",
    "# Creo un filtro con el cual selecciono las columnas coincidentes\n",
    "filtro = empresas_concatenadas_google['category'].str.contains('|'.join(restaurant_cat))\n",
    "\n",
    "# Exporto el dataframe con su nueva mascara para crear uno nuevo con los datos esperados\n",
    "empresas_concatenadas_restaurantes_estados_google = empresas_concatenadas_google[filtro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_concatenadas_restaurantes_estados_google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos los valores de categorias en formato lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "empresas_concatenadas_restaurantes_estados_google['category'] = empresas_concatenadas_restaurantes_estados_google['category'].apply(lambda x: x.replace(\"\\n\", \"\").replace(\"' '\", \"', '\").replace('\" ', '\", ').replace(' \"', ', \"').replace(',,', ','))\n",
    "empresas_concatenadas_restaurantes_estados_google['category'] = empresas_concatenadas_restaurantes_estados_google['category'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizamos los nombres de las columnas y actualizo index\n",
    "empresas_concatenadas_restaurantes_estados_google = empresas_concatenadas_restaurantes_estados_google[['gmap_id', 'name', 'latitude', 'longitude', 'stars', 'category', 'state']]\n",
    "empresas_concatenadas_restaurantes_estados_google.columns = ['gmap_id', 'name', 'latitude', 'longitude', 'stars', 'categories', 'state']\n",
    "empresas_concatenadas_restaurantes_estados_google.reset_index(drop=True, inplace=True)\n",
    "\n",
    "empresas_concatenadas_restaurantes_estados_google.to_parquet(r'..\\datasets\\processed\\google\\bussiness_google.parquet.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
