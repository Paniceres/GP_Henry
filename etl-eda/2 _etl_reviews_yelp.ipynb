{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL reviews Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización de los datasets originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo es muy grande para la tranformacion directa del archivo. Por ende lo dividimos en partes para que seas mas optimo y mas facil de trabajarlo. Lo particionamos en 300000 filas, que nos da como resultado 24 archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generamos partes de un tamaño de chunck 300000, genera 24 partes en json.\n",
    "# def dividir_json_en_partes(archivo_entrada, tamaño_maximo, prefijo_salida):\n",
    "#     with open(archivo_entrada, 'rb') as f_entrada:\n",
    "#         # Inicializa la lista para almacenar cada parte del JSON\n",
    "#         partes = []\n",
    "#         numero_parte = 1\n",
    "\n",
    "#         # Itera sobre cada línea del archivo JSON\n",
    "#         for linea_binaria in f_entrada:\n",
    "#             # Decodifica la línea binaria como UTF-8\n",
    "#             linea = linea_binaria.decode('utf-8')\n",
    "\n",
    "#             # Decodifica la línea como JSON\n",
    "#             dato = json.loads(linea)\n",
    "\n",
    "#             # Agrega el dato a la parte actual\n",
    "#             partes.append(dato)\n",
    "\n",
    "#             # Si la parte alcanza el tamaño máximo, guárdala y reinicia la lista\n",
    "#             if len(partes) >= tamaño_maximo:\n",
    "#                 guardar_parte(prefijo_salida, numero_parte, partes)\n",
    "#                 numero_parte += 1\n",
    "#                 partes = []\n",
    "\n",
    "#         # Si hay datos restantes, guárdalos como la última parte\n",
    "#         if partes:\n",
    "#             guardar_parte(prefijo_salida, numero_parte, partes)\n",
    "\n",
    "# def guardar_parte(prefijo_salida, numero_parte, datos):\n",
    "#     nombre_salida = f\"{prefijo_salida}_{numero_parte}.json\"\n",
    "#     with open(f'../extras/Datasets/Yelp/Pruebas/reviews_{nombre_salida}', 'w') as f_salida:\n",
    "#         json.dump(datos, f_salida, indent=2)  # Puedes ajustar el nivel de indentación según tus preferencias\n",
    "\n",
    "# # Especifica el archivo JSON de entrada, el tamaño máximo por parte y el prefijo para los archivos de salida\n",
    "\n",
    "# # Llama a la función para dividir el JSON en partes\n",
    "# dividir_json_en_partes('../extras/Datasets/Yelp/review.json', 300000, 'parte') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez dividido en partes lo guardamos en tipo parquet comprimido con gzip para que reduzca su tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad = list(range(1,25))\n",
    "\n",
    "# def convertir_json_a_parquet(archivo_json, archivo_parquet):\n",
    "#     # Lee el archivo JSON en un DataFrame de pandas\n",
    "#     with open(archivo_json, 'r') as f:\n",
    "#         datos_json = json.load(f)\n",
    "\n",
    "#     # Convierte el JSON a un DataFrame de pandas\n",
    "#     dataframe = pd.DataFrame(datos_json)\n",
    "\n",
    "#     # Convierte el DataFrame de pandas a una tabla de PyArrow\n",
    "#     tabla = pa.Table.from_pandas(dataframe)\n",
    "\n",
    "#     # Escribe la tabla en un archivo Parquet\n",
    "#     pq.write_table(tabla, archivo_parquet, compression='gzip')\n",
    "\n",
    "# for i in cantidad:\n",
    "#     # Especifica el archivo JSON de entrada y el archivo Parquet de salida\n",
    "#     archivo_json = f'Datasets/Yelp/Pruebas/reviews_parte_{i}.json'\n",
    "#     archivo_parquet = f'Datasets/Yelp/reviewsParquet/reviews_parte_{i}.gz.parquet'\n",
    "\n",
    "#     # Llama a la función para convertir el JSON a Parquet\n",
    "#     convertir_json_a_parquet(archivo_json, archivo_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformacion de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias necesarias, y generamos el modelo de NLP para el analisis de sentimiento de la review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download(['vader_lexicon', 'stopwords', 'punkt', 'names'])\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber si un comentario es positivo, negativo o neutro, utilizamos polarity_scores que devuelve un diccionario de 4 claves, neg, neu, pos y compound. Las que nos interesa para clasificar la reseña es el compound, que es una puntuacion compuesta de todo el texto. \n",
    "\n",
    "Para calificarlo ademas de usar esta funcion, utilizamos la clasificacion que da el resto de los usuarios, tomamos como esto a las estrellas que tiene la reseña. Por ende este valor lo dividimos por 5 asi nos quedan valores mas chicos y lo sumamos con el valor del compound previamente calculado.\n",
    "\n",
    "Generamos esta funcion puntajeNLP para poder redondear este valor calculado por la suma antes mencionada, y ahi redondearlo a 2 (positivo), 1 (negativo), 0(neutro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puntajeNLP(x):\n",
    "    if x > 1.5:\n",
    "        return 2 # Positivo\n",
    "    elif x >= 1:\n",
    "        return 0 # Neutro\n",
    "    else: \n",
    "        return 1 # Negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos la tranformacion de todas las partes realizadas anteriormente.\n",
    "\n",
    "for i in range(1,25):\n",
    "    # Leemos el archivo\n",
    "    df = pd.read_parquet(f'../extras/Datasets/Yelp/reviewsParquet/reviews_parte_{i}.gz.parquet')\n",
    "    # Nos quedamos con las columnas que necesitamos\n",
    "    df = df[['review_id', 'user_id', 'business_id','stars', 'text', 'date']]\n",
    "\n",
    "    # Realizamos el analisis de sentimiento, la division de las estrellas y la union de ambas para la clasificacion de la reseña.\n",
    "    analisis = df['text'].apply(lambda x: sid.polarity_scores(x)[\"compound\"])\n",
    "    valorEstrellas = df['stars'] / 5 \n",
    "    analisis += valorEstrellas\n",
    "    analisis = analisis.apply(lambda x: puntajeNLP(x))\n",
    "\n",
    "    # Reemplazamos la reseña por el analisis de sentimiento ya que son los datos necesarios y además para reducir el peso del archivo.\n",
    "    df['text'] = analisis\n",
    "\n",
    "    # Eliminamos stars ya que no es necesario\n",
    "    df.drop(columns='stars', inplace=True)\n",
    "\n",
    "    # Renombramos las columnas\n",
    "    df.columns = ['review_id', 'user_id', 'business_id','sentiment', 'date']\n",
    "\n",
    "    # Cambiamos tipo de dato.\n",
    "    df['sentiment'] = df['sentiment'].astype('int8')\n",
    "\n",
    "    # Exportamos el archivo para su proxima union.\n",
    "    df.to_parquet(f'../extras/Datasets optimizados/Yelp/reviews_parte_{i}_gz.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos todos los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../extras/Datasets optimizados/Yelp/reviews procecados particionado/reviews_parte_1_gz.parquet')\n",
    "for i in range(2,25):\n",
    "    # Leemos el archivo\n",
    "    aux = pd.read_parquet(f'../extras/Datasets optimizados/Yelp/reviews procecados particionado/reviews_parte_{i}_gz.parquet')\n",
    "    df = pd.concat([df,aux], axis=0, ignore_index=True)\n",
    "\n",
    "df.to_parquet('../extras/Datasets optimizados/yelp_total_reviews.gz.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos el merge con los negocios que vamos a utilizar segun el criterio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990275</th>\n",
       "      <td>H0RIamZu0B0Ei0P4aeh3sQ</td>\n",
       "      <td>qskILQ3k0I_qcCMI-k6_QQ</td>\n",
       "      <td>jals67o91gcrD4DC81Vk6w</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-12-17 21:45:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990276</th>\n",
       "      <td>shTPgbgdwTHSuU67mGCmZQ</td>\n",
       "      <td>Zo0th2m8Ez4gLSbHftiQvg</td>\n",
       "      <td>2vLksaMmSEcGbjI5gywpZA</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-31 16:55:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990277</th>\n",
       "      <td>YNfNhgZlaaCO5Q_YJR4rEw</td>\n",
       "      <td>mm6E4FbCMwJmb7kPDZ5v2Q</td>\n",
       "      <td>R1khUUxidqfaJmcpmGd4aw</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-30 03:56:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990278</th>\n",
       "      <td>i-I4ZOhoX70Nw5H0FwrQUA</td>\n",
       "      <td>YwAMC-jvZ1fvEUum6QkEkw</td>\n",
       "      <td>Rr9kKArrMhSLVE9a53q-aA</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-19 18:59:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990279</th>\n",
       "      <td>RwcKOdEuLRHNJe4M9-qpqg</td>\n",
       "      <td>6JehEvdoCvZPJ_XIxnzIIw</td>\n",
       "      <td>VAeEXLbEcI9Emt9KGYq9aA</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-02 22:50:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      review_id                 user_id  \\\n",
       "0        KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n",
       "1        BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n",
       "2        saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n",
       "3        AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n",
       "4        Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n",
       "...                         ...                     ...   \n",
       "6990275  H0RIamZu0B0Ei0P4aeh3sQ  qskILQ3k0I_qcCMI-k6_QQ   \n",
       "6990276  shTPgbgdwTHSuU67mGCmZQ  Zo0th2m8Ez4gLSbHftiQvg   \n",
       "6990277  YNfNhgZlaaCO5Q_YJR4rEw  mm6E4FbCMwJmb7kPDZ5v2Q   \n",
       "6990278  i-I4ZOhoX70Nw5H0FwrQUA  YwAMC-jvZ1fvEUum6QkEkw   \n",
       "6990279  RwcKOdEuLRHNJe4M9-qpqg  6JehEvdoCvZPJ_XIxnzIIw   \n",
       "\n",
       "                    business_id  sentiment                 date  \n",
       "0        XQfwVwDr-v0ZS3_CbbE5Xw          0  2018-07-07 22:09:11  \n",
       "1        7ATYjTIgM3jUlt4UM3IypQ          2  2012-01-03 15:28:18  \n",
       "2        YjUWPpI6HXG530lwP-fb2A          2  2014-02-05 20:30:30  \n",
       "3        kxX2SOes4o-D3ZQBkiMRfA          2  2015-01-04 00:01:03  \n",
       "4        e4Vwtrqf-wpJfwesgvdgxQ          2  2017-01-14 20:54:15  \n",
       "...                         ...        ...                  ...  \n",
       "6990275  jals67o91gcrD4DC81Vk6w          0  2014-12-17 21:45:20  \n",
       "6990276  2vLksaMmSEcGbjI5gywpZA          2  2021-03-31 16:55:10  \n",
       "6990277  R1khUUxidqfaJmcpmGd4aw          0  2019-12-30 03:56:30  \n",
       "6990278  Rr9kKArrMhSLVE9a53q-aA          2  2022-01-19 18:59:27  \n",
       "6990279  VAeEXLbEcI9Emt9KGYq9aA          2  2018-01-02 22:50:47  \n",
       "\n",
       "[6990280 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewsDf = pd.read_parquet('../extras/Datasets optimizados/Yelp/yelp_total_reviews.gz.parquet')\n",
    "reviewsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>categories</th>\n",
       "      <th>Estado_Perteneciente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qkRM_2X51Yqxk3btlwAQIg</td>\n",
       "      <td>Temple Beth-El</td>\n",
       "      <td>27.766590</td>\n",
       "      <td>-82.732983</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Synagogues, Religious Organizations</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UJsufbvfyfONHeWdvAHKjA</td>\n",
       "      <td>Marshalls</td>\n",
       "      <td>28.190459</td>\n",
       "      <td>-82.457380</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Department Stores, Shopping, Fashion</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eEOYSgkmpB90uNA7lDOMRA</td>\n",
       "      <td>Vietnamese Food Truck</td>\n",
       "      <td>27.955269</td>\n",
       "      <td>-82.456320</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Vietnamese, Food, Restaurants, Food Trucks</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaxMSoInw8Poo3XeMJt8lQ</td>\n",
       "      <td>Adams Dental</td>\n",
       "      <td>27.966235</td>\n",
       "      <td>-82.787412</td>\n",
       "      <td>5.0</td>\n",
       "      <td>General Dentistry, Dentists, Health &amp; Medical,...</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0bPLkL0QhhPO5kt1_EXmNQ</td>\n",
       "      <td>Zio's Italian Market</td>\n",
       "      <td>27.916116</td>\n",
       "      <td>-82.760461</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Food, Delis, Italian, Bakeries, Restaurants</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41323</th>\n",
       "      <td>paqoi156XSa8BaIR70gR8g</td>\n",
       "      <td>Franklinville Family &amp; Cosmetic Dentistry</td>\n",
       "      <td>39.606158</td>\n",
       "      <td>-75.070777</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Teeth Whitening, Endodontists, Beauty &amp; Spas, ...</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41324</th>\n",
       "      <td>lglUHT1ByJh_frd1kLKhyQ</td>\n",
       "      <td>Hollywood Nails</td>\n",
       "      <td>39.956757</td>\n",
       "      <td>-75.068777</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Nail Salons, Beauty &amp; Spas</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41325</th>\n",
       "      <td>-a7VXX0-V9LgWMFrq90iNA</td>\n",
       "      <td>Delran Auto Body</td>\n",
       "      <td>40.021346</td>\n",
       "      <td>-74.932089</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Body Shops, Automotive</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41326</th>\n",
       "      <td>_h9b34onQc_26F9mvmsNhw</td>\n",
       "      <td>J&amp;M Gutter Pros</td>\n",
       "      <td>39.851945</td>\n",
       "      <td>-74.961517</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Home Services, Pressure Washers, Gutter Servic...</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41327</th>\n",
       "      <td>x_2IrYgFiQn7GOTTgWRbAw</td>\n",
       "      <td>The Vac &amp; Sew Center</td>\n",
       "      <td>39.857700</td>\n",
       "      <td>-74.987230</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Appliances &amp; Repair, Home &amp; Garden, Appliances...</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41295 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id                                       name  \\\n",
       "0      qkRM_2X51Yqxk3btlwAQIg                             Temple Beth-El   \n",
       "1      UJsufbvfyfONHeWdvAHKjA                                  Marshalls   \n",
       "2      eEOYSgkmpB90uNA7lDOMRA                      Vietnamese Food Truck   \n",
       "3      jaxMSoInw8Poo3XeMJt8lQ                               Adams Dental   \n",
       "4      0bPLkL0QhhPO5kt1_EXmNQ                       Zio's Italian Market   \n",
       "...                       ...                                        ...   \n",
       "41323  paqoi156XSa8BaIR70gR8g  Franklinville Family & Cosmetic Dentistry   \n",
       "41324  lglUHT1ByJh_frd1kLKhyQ                            Hollywood Nails   \n",
       "41325  -a7VXX0-V9LgWMFrq90iNA                           Delran Auto Body   \n",
       "41326  _h9b34onQc_26F9mvmsNhw                            J&M Gutter Pros   \n",
       "41327  x_2IrYgFiQn7GOTTgWRbAw                       The Vac & Sew Center   \n",
       "\n",
       "        latitude  longitude  stars  \\\n",
       "0      27.766590 -82.732983    3.5   \n",
       "1      28.190459 -82.457380    3.5   \n",
       "2      27.955269 -82.456320    4.0   \n",
       "3      27.966235 -82.787412    5.0   \n",
       "4      27.916116 -82.760461    4.5   \n",
       "...          ...        ...    ...   \n",
       "41323  39.606158 -75.070777    1.5   \n",
       "41324  39.956757 -75.068777    3.0   \n",
       "41325  40.021346 -74.932089    3.5   \n",
       "41326  39.851945 -74.961517    5.0   \n",
       "41327  39.857700 -74.987230    4.0   \n",
       "\n",
       "                                              categories Estado_Perteneciente  \n",
       "0                    Synagogues, Religious Organizations              Florida  \n",
       "1                   Department Stores, Shopping, Fashion              Florida  \n",
       "2             Vietnamese, Food, Restaurants, Food Trucks              Florida  \n",
       "3      General Dentistry, Dentists, Health & Medical,...              Florida  \n",
       "4            Food, Delis, Italian, Bakeries, Restaurants              Florida  \n",
       "...                                                  ...                  ...  \n",
       "41323  Teeth Whitening, Endodontists, Beauty & Spas, ...           New Jersey  \n",
       "41324                         Nail Salons, Beauty & Spas           New Jersey  \n",
       "41325                             Body Shops, Automotive           New Jersey  \n",
       "41326  Home Services, Pressure Washers, Gutter Servic...           New Jersey  \n",
       "41327  Appliances & Repair, Home & Garden, Appliances...           New Jersey  \n",
       "\n",
       "[41295 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negociosDf = pd.read_json('../datasets/processed/yelp/bussiness_yelp.json.gz')\n",
    "negociosDf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
