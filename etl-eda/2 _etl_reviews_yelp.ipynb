{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL reviews Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización de los datasets originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo es muy grande para la tranformacion directa del mismo. Por ende lo dividimos en partes para que seas mas optimo y mas facil de trabajarlo. Lo particionamos sera de 300000 filas, que nos da como resultado 24 archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generamos partes de un tamaño de chunck 300000, genera 24 partes en json.\n",
    "# def dividir_json_en_partes(archivo_entrada, tamaño_maximo, prefijo_salida):\n",
    "#     with open(archivo_entrada, 'rb') as f_entrada:\n",
    "#         # Inicializa la lista para almacenar cada parte del JSON\n",
    "#         partes = []\n",
    "#         numero_parte = 1\n",
    "\n",
    "#         # Itera sobre cada línea del archivo JSON\n",
    "#         for linea_binaria in f_entrada:\n",
    "#             # Decodifica la línea binaria como UTF-8\n",
    "#             linea = linea_binaria.decode('utf-8')\n",
    "\n",
    "#             # Decodifica la línea como JSON\n",
    "#             dato = json.loads(linea)\n",
    "\n",
    "#             # Agrega el dato a la parte actual\n",
    "#             partes.append(dato)\n",
    "\n",
    "#             # Si la parte alcanza el tamaño máximo, guárdala y reinicia la lista\n",
    "#             if len(partes) >= tamaño_maximo:\n",
    "#                 guardar_parte(prefijo_salida, numero_parte, partes)\n",
    "#                 numero_parte += 1\n",
    "#                 partes = []\n",
    "\n",
    "#         # Si hay datos restantes, guárdalos como la última parte\n",
    "#         if partes:\n",
    "#             guardar_parte(prefijo_salida, numero_parte, partes)\n",
    "\n",
    "# def guardar_parte(prefijo_salida, numero_parte, datos):\n",
    "#     nombre_salida = f\"{prefijo_salida}_{numero_parte}.json\"\n",
    "#     with open(f'../extras/Datasets/Yelp/Pruebas/reviews_{nombre_salida}', 'w') as f_salida:\n",
    "#         json.dump(datos, f_salida, indent=2)  # Puedes ajustar el nivel de indentación según tus preferencias\n",
    "\n",
    "# # Especifica el archivo JSON de entrada, el tamaño máximo por parte y el prefijo para los archivos de salida\n",
    "\n",
    "# # Llama a la función para dividir el JSON en partes\n",
    "# dividir_json_en_partes('../extras/Datasets/Yelp/review.json', 300000, 'parte') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez dividido en partes lo guardamos en tipo parquet comprimido con gzip para que reduzca su tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad = list(range(1,25))\n",
    "\n",
    "# def convertir_json_a_parquet(archivo_json, archivo_parquet):\n",
    "#     # Lee el archivo JSON en un DataFrame de pandas\n",
    "#     with open(archivo_json, 'r') as f:\n",
    "#         datos_json = json.load(f)\n",
    "\n",
    "#     # Convierte el JSON a un DataFrame de pandas\n",
    "#     dataframe = pd.DataFrame(datos_json)\n",
    "\n",
    "#     # Convierte el DataFrame de pandas a una tabla de PyArrow\n",
    "#     tabla = pa.Table.from_pandas(dataframe)\n",
    "\n",
    "#     # Escribe la tabla en un archivo Parquet\n",
    "#     pq.write_table(tabla, archivo_parquet, compression='gzip')\n",
    "\n",
    "# for i in cantidad:\n",
    "#     # Especifica el archivo JSON de entrada y el archivo Parquet de salida\n",
    "#     archivo_json = f'Datasets/Yelp/Pruebas/reviews_parte_{i}.json'\n",
    "#     archivo_parquet = f'Datasets/Yelp/reviewsParquet/reviews_parte_{i}.gz.parquet'\n",
    "\n",
    "#     # Llama a la función para convertir el JSON a Parquet\n",
    "#     convertir_json_a_parquet(archivo_json, archivo_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformacion de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias necesarias, y generamos el modelo de NLP para el analisis de sentimiento de la review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download(['vader_lexicon', 'stopwords', 'punkt', 'names'])\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber si un comentario es positivo, negativo o neutro, utilizamos polarity_scores que devuelve un diccionario de 4 claves, neg, neu, pos y compound. Las que nos interesa para clasificar la reseña es el compound, que es una puntuacion compuesta de todo el texto. \n",
    "\n",
    "Para calificarlo ademas de usar esta funcion, utilizamos la clasificacion que da el resto de los usuarios, tomamos como esto a las estrellas que tiene la reseña. Por ende este valor lo dividimos por 5 asi nos quedan valores mas chicos y lo sumamos con el valor del compound previamente calculado.\n",
    "\n",
    "Generamos esta funcion puntajeNLP para poder redondear este valor calculado por la suma antes mencionada, y ahi redondearlo a 2 (positivo), 1 (negativo), 0(neutro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puntajeNLP(x):\n",
    "    if x > 1.5:\n",
    "        return 2 # Positivo\n",
    "    elif x >= 1:\n",
    "        return 0 # Neutro\n",
    "    else: \n",
    "        return 1 # Negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos la tranformacion de todas las partes realizadas anteriormente.\n",
    "\n",
    "for i in range(1,25):\n",
    "    # Leemos el archivo\n",
    "    df = pd.read_parquet(f'../extras/Datasets/Yelp/reviewsParquet/reviews_parte_{i}.gz.parquet')\n",
    "    # Nos quedamos con las columnas que necesitamos\n",
    "    df = df[['review_id', 'user_id', 'business_id','stars', 'text', 'date']]\n",
    "\n",
    "    # Realizamos el analisis de sentimiento, la division de las estrellas y la suma de ambas para la clasificacion de la reseña.\n",
    "    analisis = df['text'].apply(lambda x: sid.polarity_scores(x)[\"compound\"])\n",
    "    valorEstrellas = df['stars'] / 5 \n",
    "    analisis += valorEstrellas\n",
    "    analisis = analisis.apply(lambda x: puntajeNLP(x))\n",
    "\n",
    "    # Reemplazamos la reseña por el analisis de sentimiento ya que son los datos necesarios y además para reducir el peso del archivo.\n",
    "    df['text'] = analisis\n",
    "\n",
    "    # Eliminamos stars ya que no es necesario\n",
    "    df.drop(columns='stars', inplace=True)\n",
    "\n",
    "    # Renombramos las columnas\n",
    "    df.columns = ['review_id', 'user_id', 'business_id','sentiment', 'date']\n",
    "\n",
    "    # Cambiamos tipo de dato.\n",
    "    df['sentiment'] = df['sentiment'].astype('int8')\n",
    "\n",
    "    # Exportamos el archivo para su proxima union.\n",
    "    df.to_parquet(f'../extras/Datasets optimizados/Yelp/reviews_parte_{i}_gz.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos todos los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../extras/Datasets optimizados/Yelp/reviews procecados particionado/reviews_parte_1_gz.parquet')\n",
    "for i in range(2,25):\n",
    "    # Leemos el archivo\n",
    "    aux = pd.read_parquet(f'../extras/Datasets optimizados/Yelp/reviews procecados particionado/reviews_parte_{i}_gz.parquet')\n",
    "    df = pd.concat([df,aux], axis=0, ignore_index=True)\n",
    "\n",
    "df.to_parquet('../extras/Datasets optimizados/yelp_total_reviews.gz.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos el merge con los negocios que vamos a utilizar segun el criterio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsDf = pd.read_parquet('../extras/Datasets optimizados/Yelp/yelp_total_reviews.gz.parquet')\n",
    "\n",
    "# Dataset de negocios ya filtrado por estado y tipo de negocio.\n",
    "negociosDf = pd.read_json('../datasets/processed/yelp/bussiness_yelp.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos el dataset de las reviews que vamos a utilizar, y lo guardamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsDf = reviewsDf.merge(negociosDf['business_id'], how='inner')\n",
    "reviewsDf.to_parquet('../datasets/processed/yelp/reviews_yelp.parquet.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
