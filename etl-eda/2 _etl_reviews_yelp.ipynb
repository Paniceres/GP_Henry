{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL reviews Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización de los datasets originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo es muy grande para la tranformacion directa del archivo. Por ende lo dividimos en partes para que seas mas optimo y mas facil de trabajarlo. Lo particionamos en 300000 filas, que nos da como resultado 24 archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generamos partes de un tamaño de chunck 300000, genera 24 partes en json.\n",
    "# def dividir_json_en_partes(archivo_entrada, tamaño_maximo, prefijo_salida):\n",
    "#     with open(archivo_entrada, 'rb') as f_entrada:\n",
    "#         # Inicializa la lista para almacenar cada parte del JSON\n",
    "#         partes = []\n",
    "#         numero_parte = 1\n",
    "\n",
    "#         # Itera sobre cada línea del archivo JSON\n",
    "#         for linea_binaria in f_entrada:\n",
    "#             # Decodifica la línea binaria como UTF-8\n",
    "#             linea = linea_binaria.decode('utf-8')\n",
    "\n",
    "#             # Decodifica la línea como JSON\n",
    "#             dato = json.loads(linea)\n",
    "\n",
    "#             # Agrega el dato a la parte actual\n",
    "#             partes.append(dato)\n",
    "\n",
    "#             # Si la parte alcanza el tamaño máximo, guárdala y reinicia la lista\n",
    "#             if len(partes) >= tamaño_maximo:\n",
    "#                 guardar_parte(prefijo_salida, numero_parte, partes)\n",
    "#                 numero_parte += 1\n",
    "#                 partes = []\n",
    "\n",
    "#         # Si hay datos restantes, guárdalos como la última parte\n",
    "#         if partes:\n",
    "#             guardar_parte(prefijo_salida, numero_parte, partes)\n",
    "\n",
    "# def guardar_parte(prefijo_salida, numero_parte, datos):\n",
    "#     nombre_salida = f\"{prefijo_salida}_{numero_parte}.json\"\n",
    "#     with open(f'../extras/Datasets/Yelp/Pruebas/reviews_{nombre_salida}', 'w') as f_salida:\n",
    "#         json.dump(datos, f_salida, indent=2)  # Puedes ajustar el nivel de indentación según tus preferencias\n",
    "\n",
    "# # Especifica el archivo JSON de entrada, el tamaño máximo por parte y el prefijo para los archivos de salida\n",
    "\n",
    "# # Llama a la función para dividir el JSON en partes\n",
    "# dividir_json_en_partes('../extras/Datasets/Yelp/review.json', 300000, 'parte') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez dividido en partes lo guardamos en tipo parquet comprimido con gzip para que reduzca su tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad = list(range(1,25))\n",
    "\n",
    "# def convertir_json_a_parquet(archivo_json, archivo_parquet):\n",
    "#     # Lee el archivo JSON en un DataFrame de pandas\n",
    "#     with open(archivo_json, 'r') as f:\n",
    "#         datos_json = json.load(f)\n",
    "\n",
    "#     # Convierte el JSON a un DataFrame de pandas\n",
    "#     dataframe = pd.DataFrame(datos_json)\n",
    "\n",
    "#     # Convierte el DataFrame de pandas a una tabla de PyArrow\n",
    "#     tabla = pa.Table.from_pandas(dataframe)\n",
    "\n",
    "#     # Escribe la tabla en un archivo Parquet\n",
    "#     pq.write_table(tabla, archivo_parquet, compression='gzip')\n",
    "\n",
    "# for i in cantidad:\n",
    "#     # Especifica el archivo JSON de entrada y el archivo Parquet de salida\n",
    "#     archivo_json = f'Datasets/Yelp/Pruebas/reviews_parte_{i}.json'\n",
    "#     archivo_parquet = f'Datasets/Yelp/reviewsParquet/reviews_parte_{i}.gz.parquet'\n",
    "\n",
    "#     # Llama a la función para convertir el JSON a Parquet\n",
    "#     convertir_json_a_parquet(archivo_json, archivo_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformacion de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias necesarias, y generamos el modelo de NLP para el analisis de sentimiento de la review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Damian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download(['vader_lexicon', 'stopwords', 'punkt', 'names'])\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber si un comentario es positivo, negativo o neutro, utilizamos polarity_scores que devuelve un diccionario de 4 claves, neg, neu, pos y compound. Las que nos interesa para clasificar la reseña es el compound, que es una puntuacion compuesta de todo el texto. \n",
    "\n",
    "Para calificarlo ademas de usar esta funcion, utilizamos la clasificacion que da el resto de los usuarios, tomamos como esto a las estrellas que tiene la reseña. Por ende este valor lo dividimos por 5 asi nos quedan valores mas chicos y lo sumamos con el valor del compound previamente calculado.\n",
    "\n",
    "Generamos esta funcion puntajeNLP para poder redondear este valor calculado por la suma antes mencionada, y ahi redondearlo a 2 (positivo), 1 (negativo), 0(neutro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puntajeNLP(x):\n",
    "    if x > 1.5:\n",
    "        return 2 # Positivo\n",
    "    elif x >= 1:\n",
    "        return 0 # Neutro\n",
    "    else: \n",
    "        return 1 # Negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos la tranformacion de todas las partes realizadas anteriormente.\n",
    "\n",
    "for i in range(1,25):\n",
    "    # Leemos el archivo\n",
    "    df = pd.read_parquet(f'../extras/Datasets/Yelp/reviewsParquet/reviews_parte_{i}.gz.parquet')\n",
    "    # Nos quedamos con las columnas que necesitamos\n",
    "    df = df[['review_id', 'user_id', 'business_id','stars', 'text', 'date']]\n",
    "\n",
    "    # Realizamos el analisis de sentimiento, la division de las estrellas y la union de ambas para la clasificacion de la reseña.\n",
    "    analisis = df['text'].apply(lambda x: sid.polarity_scores(x)[\"compound\"])\n",
    "    valorEstrellas = df['stars'] / 5 \n",
    "    analisis += valorEstrellas\n",
    "    analisis = analisis.apply(lambda x: puntajeNLP(x))\n",
    "\n",
    "    # Reemplazamos la reseña por el analisis de sentimiento ya que son los datos necesarios y además para reducir el peso del archivo.\n",
    "    df['text'] = analisis\n",
    "\n",
    "    # Eliminamos stars ya que no es necesario\n",
    "    df.drop(columns='stars', inplace=True)\n",
    "\n",
    "    # Renombramos las columnas\n",
    "    df.columns = ['review_id', 'user_id', 'business_id','sentiment', 'date']\n",
    "\n",
    "    # Cambiamos tipo de dato.\n",
    "    df['sentiment'] = df['sentiment'].astype('int8')\n",
    "\n",
    "    # Exportamos el archivo para su proxima union.\n",
    "    df.to_parquet(f'../extras/Datasets optimizados/Yelp/reviews_parte_{i}_gz.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos todos los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990275</th>\n",
       "      <td>H0RIamZu0B0Ei0P4aeh3sQ</td>\n",
       "      <td>qskILQ3k0I_qcCMI-k6_QQ</td>\n",
       "      <td>jals67o91gcrD4DC81Vk6w</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-12-17 21:45:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990276</th>\n",
       "      <td>shTPgbgdwTHSuU67mGCmZQ</td>\n",
       "      <td>Zo0th2m8Ez4gLSbHftiQvg</td>\n",
       "      <td>2vLksaMmSEcGbjI5gywpZA</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-31 16:55:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990277</th>\n",
       "      <td>YNfNhgZlaaCO5Q_YJR4rEw</td>\n",
       "      <td>mm6E4FbCMwJmb7kPDZ5v2Q</td>\n",
       "      <td>R1khUUxidqfaJmcpmGd4aw</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-30 03:56:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990278</th>\n",
       "      <td>i-I4ZOhoX70Nw5H0FwrQUA</td>\n",
       "      <td>YwAMC-jvZ1fvEUum6QkEkw</td>\n",
       "      <td>Rr9kKArrMhSLVE9a53q-aA</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-19 18:59:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990279</th>\n",
       "      <td>RwcKOdEuLRHNJe4M9-qpqg</td>\n",
       "      <td>6JehEvdoCvZPJ_XIxnzIIw</td>\n",
       "      <td>VAeEXLbEcI9Emt9KGYq9aA</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-02 22:50:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      review_id                 user_id  \\\n",
       "0        KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n",
       "1        BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n",
       "2        saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n",
       "3        AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n",
       "4        Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n",
       "...                         ...                     ...   \n",
       "6990275  H0RIamZu0B0Ei0P4aeh3sQ  qskILQ3k0I_qcCMI-k6_QQ   \n",
       "6990276  shTPgbgdwTHSuU67mGCmZQ  Zo0th2m8Ez4gLSbHftiQvg   \n",
       "6990277  YNfNhgZlaaCO5Q_YJR4rEw  mm6E4FbCMwJmb7kPDZ5v2Q   \n",
       "6990278  i-I4ZOhoX70Nw5H0FwrQUA  YwAMC-jvZ1fvEUum6QkEkw   \n",
       "6990279  RwcKOdEuLRHNJe4M9-qpqg  6JehEvdoCvZPJ_XIxnzIIw   \n",
       "\n",
       "                    business_id  sentiment                 date  \n",
       "0        XQfwVwDr-v0ZS3_CbbE5Xw          0  2018-07-07 22:09:11  \n",
       "1        7ATYjTIgM3jUlt4UM3IypQ          2  2012-01-03 15:28:18  \n",
       "2        YjUWPpI6HXG530lwP-fb2A          2  2014-02-05 20:30:30  \n",
       "3        kxX2SOes4o-D3ZQBkiMRfA          2  2015-01-04 00:01:03  \n",
       "4        e4Vwtrqf-wpJfwesgvdgxQ          2  2017-01-14 20:54:15  \n",
       "...                         ...        ...                  ...  \n",
       "6990275  jals67o91gcrD4DC81Vk6w          0  2014-12-17 21:45:20  \n",
       "6990276  2vLksaMmSEcGbjI5gywpZA          2  2021-03-31 16:55:10  \n",
       "6990277  R1khUUxidqfaJmcpmGd4aw          0  2019-12-30 03:56:30  \n",
       "6990278  Rr9kKArrMhSLVE9a53q-aA          2  2022-01-19 18:59:27  \n",
       "6990279  VAeEXLbEcI9Emt9KGYq9aA          2  2018-01-02 22:50:47  \n",
       "\n",
       "[6990280 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('../extras/Datasets optimizados/Yelp/reviews_parte_1_gz.parquet')\n",
    "for i in range(2,25):\n",
    "    # Leemos el archivo\n",
    "    aux = pd.read_parquet(f'../extras/Datasets optimizados/Yelp/reviews_parte_{i}_gz.parquet')\n",
    "    df = pd.concat([df,aux], axis=0, ignore_index=True)\n",
    "\n",
    "df.to_parquet('../extras/Datasets optimizados/yelp_total_reviews.gz.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
